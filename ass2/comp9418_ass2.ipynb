{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 (Practical)\n",
    "\n",
    "**COMP9418 - Advanced Topics in Statistical Machine Learning**\n",
    "\n",
    "**Louis Tiao** (TA), **Edwin V. Bonilla** (Instructor)\n",
    "\n",
    "*School of Computer Science and Engineering, UNSW Sydney*\n",
    "\n",
    "---\n",
    "\n",
    "In the practical component of this assignment you will build a *class-conditional classifier* using the mixture model described in the theory section of this assignment.\n",
    "\n",
    "The basic idea behind a class conditional classifier is to train a separate model for each class $p(\\mathbf{x} \\mid y)$, and use Bayes' rule to classify a novel data-point $\\mathbf{x}^*$ with:\n",
    "\n",
    "$$\n",
    "p(y^* \\mid \\mathbf{x}^*) = \\frac{p(\\mathbf{x}^* \\mid y^*) p(y^*)}{\\sum_{y'=1}^C p(\\mathbf{x}^* \\mid y') p(y')}\n",
    "$$\n",
    "\n",
    "(c.f. Barber textbook BRML, 2012, $\\S$23.3.4 or Murphy textbook MLaPP, 2012, $\\S$17.5.4).\n",
    "\n",
    "In this assignment, you will use the prescribed mixture model for each of the conditional densities $p(\\mathbf{x} | y)$ and a Categorical distribution for $p(y)$.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "You will require the following packages for this assignment:\n",
    "\n",
    "- `numpy`\n",
    "- `scipy`\n",
    "- `scikit-learn`\n",
    "- `matplotlib`\n",
    "- `observations`\n",
    "\n",
    "Most of these may be installed with `pip`:\n",
    "\n",
    "    pip install numpy scipy scikit-learn matplotlib observations\n",
    "\n",
    "### Guidelines\n",
    "\n",
    "1. Unless otherwise indicated, you may not use any ML libraries and frameworks such as scikit-learn, TensorFlow to implement any training-related code. Your solution should be implement purely in NumPy/SciPy.\n",
    "2. Do not delete any of the existing code-blocks in this notebook. It will be used to assess the performance of your algorithm.\n",
    "\n",
    "### Assessment\n",
    "\n",
    "Your work will be assessed based on:\n",
    "- **[50%]** the application of the concepts for doing model selection, which allows you to learn a single model for prediction (Section 1);  \n",
    "- **[30%]** the code you write for making predicitions in your model (Section 2); and\n",
    "- **[20%]** the predictive performance of your model (Section 3). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "You will be building a class-conditional classifier to classify digits from the [Fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist), containing grayscale images of clothing items --- coats, shirts, sneakers, dresses and the like.\n",
    "\n",
    "This can be obtained with [observations](https://github.com/edwardlib/observations), a convenient tool for loading standard ML datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from observations import fashion_mnist\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train_), _ = fashion_mnist('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 60k training examples, each consisting of 784-dimensional feature vectors corresponding to 28 x 28 pixel intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pixel intensities are originally unsigned 8-bit integers (`uint8`) and should be normalized to be floating-point decimals within range $[0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sklearn' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e1ba75b6c011>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mreducer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mreducer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreducer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sklearn' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "x_train = x_train / 255.\n",
    "x_train=x_train[:2000]\n",
    "reducer=sklearn.decomposition.PCA(n_components=100)\n",
    "reducer.fit(x_train)\n",
    "x_train=reducer.transform(x_train)\n",
    "print (x_train[0],x_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The targets contain the class label corresponding to each example. For this assignment, you should represent this using the \"one-hot\" encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = LabelBinarizer().fit_transform(y_train_)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you are only to use the training data contained in `x_train`, `y_train` as we have define it. In order to learn and test you model, you may consider splitting these data into training, validation and testing. You may not use any other data to for training.\n",
    "\n",
    "In particular, if you want to assess the performance of your model in section 2, you must create a test set `test.npz`. You are not required to submit this test file as we will evaluate the performance of your model using our own test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the function below to plot a digits in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_grid(ax, images, n=20, m=None, img_rows=28, img_cols=28):\n",
    "    \"\"\"\n",
    "    Plot the first `n * m` vectors in the array as \n",
    "    a `n`-by-`m` grid of `img_rows`-by-`img_cols` images.\n",
    "    \"\"\"\n",
    "    if m is None:\n",
    "        m = n\n",
    " \n",
    "    grid = images[:n*m].reshape(n, m, img_rows, img_cols)\n",
    "\n",
    "    return ax.imshow(np.vstack(np.dstack(grid)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the first 400 images in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "plot_image_grid(ax, x_train, n=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the first 400 images labeled \"t-shirts\" in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "plot_image_grid(ax, x_train[y_train_ == 0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 `[50%]`: Model Training\n",
    "\n",
    "Place all the code for training your model using the function `model_train` below. \n",
    "\n",
    "- We should be able to run your notebook (by clicking 'Cell->Run All') without errors. However, you must save the trained model in the file `model.npz`. This file will be loaded to make predictions in section 2 and assess the performance of your model in section 3. Note that, in addition to this notebook file, <span style=\"color:red\"> ** you must provide the file `model.npz` **</span>.\n",
    "\n",
    "- You should comment your code as much as possible so we understand your reasoning about training, model selection and avoiding overfitting. \n",
    "\n",
    "- You can process the data as you wish, e.g. by applying some additional transformations, reducing dimensionality, etc. However, all these should be here too. \n",
    "\n",
    "- Wrap all your training using the function `model_train` below. You can call all other custom functions within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussian(data,mean,cov):\n",
    "    dim = np.shape(data)[0]\n",
    "    # print('weidu',dim)# 计算维度!!!!!!!!!!!!!!!!!\n",
    "    covdet = np.linalg.det(cov) # 计算|cov|\n",
    "    # print(covdet)\n",
    "    covinv = np.linalg.inv(cov) # 计算cov的逆\n",
    "    # print(covinv.shape)\n",
    "    if covdet==0.0 and covdet==-0.0:              # 以防行列式为0\n",
    "        covdet = np.linalg.det(cov+np.eye(dim)*0.01)\n",
    "        covdet = 1\n",
    "        covinv = np.linalg.inv(cov+np.eye(dim)*0.01)\n",
    "        ##矩阵求逆\n",
    "    diff = data - mean\n",
    "    z = -0.5 * np.multiply(np.multiply(diff, covinv),diff.T)\n",
    "    # print('z',np.power(2*np.pi,784))# 计算exp()里的值\n",
    "    return 1.0 / (np.power(np.power(2 * np.pi, dim) * abs(covdet), 0.5)) * np.exp(z)\n",
    "\n",
    "\n",
    "def GetInitialMeans(data,K):\n",
    "    dim = data.shape[1]  # 数据的维度\n",
    "    means = [[] for k in range(K)]\n",
    "    # 存储均值\n",
    "    minmax=[]\n",
    "    for i in range(dim):\n",
    "        minmax.append(np.array([min(data[:,i]),max(data[:,i])]))  # 存储每一维的最大最小值\n",
    "        # print('每一维的最大最小值', minmax)\n",
    "    minmax=np.array(minmax)\n",
    "    for i in range(K):\n",
    "        means[i]=[]\n",
    "        for j in range(dim):\n",
    "            means[i].append(np.random.random()*(minmax[i][1]-minmax[i][0])+minmax[i][0] ) #随机产生means,每一维最大减最小乘个0.几的随机数，再加上最小\n",
    "        means[i]=np.array(means[i])\n",
    "            # print(means[i])\n",
    "    return means\n",
    "def model_train(x_train, y_train):\n",
    "    \"\"\"\n",
    "    Write your code here.\n",
    "    \"\"\"\n",
    "    data=x_train\n",
    "    D,N=data.shape##60000,784\n",
    "    # print(D,N)\n",
    "    # print(data.shape)\n",
    "    # print(y_train.shape)\n",
    "    K=y_train.shape[1]###10类\n",
    "    pai=np.array([1]*K)/K\n",
    "    Q=30\n",
    "    bias=0.0000000000001\n",
    "\n",
    "\n",
    "    Z=np.array(np.random.normal(loc=0,scale=0.1,size=Q).reshape([Q,1]))##对于隐变量\n",
    "    miu=np.array(np.random.normal(loc=0, scale=0.1, size=K*N)).reshape([K,N,])\n",
    "    W = np.array(np.random.normal(loc=0, scale=0.1, size=K*N*Q)).reshape([K,N,Q])\n",
    "    psi=np.eye(N)\n",
    "    # print ('QWEQWEQWE',psi.shape)\n",
    "    # print('fangcha',psi.shape)\n",
    "    # convs=np.linalg.inv(pis)\n",
    "    # print(convs)\n",
    "    # for i in range(K):\n",
    "    #     convs[i] = np.cov(data.T)\n",
    "    # # convs[np.eye(5)] for k in range(K)\n",
    "    # print('方差的维度',convs.shape())\n",
    "    # #E\n",
    "    # rnk = [np.zeros(K) for i in range(N)]\n",
    "    # print(gammas[0][0],gammas[N-1][K-1])\n",
    "    r=[]\n",
    "    R=[]\n",
    "    newloglikelyhood=0\n",
    "    oldloglikelyhood=1\n",
    "    rnk = np.array([np.zeros(K) for i in range(N)])###初始rnk表\n",
    "    print (rnk.shape)\n",
    "    # while np.abs(oldloglikelyhood-newloglikelyhood)>0.0001:  ###10类\n",
    "    while True:\n",
    "        print ('****')\n",
    "        sum=0\n",
    "        ##-----------EEEE-step----------------##\n",
    "        for i in range(N):\n",
    "            rnk_list=[]\n",
    "            for k in range(K):\n",
    "                # print ('Wk',W[k].shape)\n",
    "                tem=psi + np.matmul(W[k] , W[k].T)\n",
    "                # print (np.count_nonzero(tem))\n",
    "                if np.linalg.det(tem)==0:\n",
    "                    tem = np.where(tem == 0, bias, tem)\n",
    "                    # tem[0][0]=tem[0][0]+bias*0.01\n",
    "                    # print (tem[0][0])\n",
    "                    # print (np.count_nonzero(tem))\n",
    "\n",
    "                else:\n",
    "                    tem=tem\n",
    "\n",
    "                beta_k = np.matmul(W[k].T , np.linalg.inv(tem))\n",
    "                #print ('beta', beta_k)\n",
    "                E = np.sum( pai[k]* Gaussian(data[i],data[i]-miu[k], (np.matmul(W[k],W[k].T)+psi)))\n",
    "                # print ('EEEEEEE',E)\n",
    "                # np.random.rand(1)\n",
    "                rnk_list.append(E)\n",
    "                # print(miu[k].shape)\n",
    "                Ez_w__x=np.matmul(beta_k,(data[i]-miu[k]))\n",
    "                diff=data[i]-miu[k]\n",
    "                Ez_w__x=Ez_w__x.reshape(Ez_w__x.shape[0],1)\n",
    "                diff=diff.reshape(data[i].shape[0],1)\n",
    "\n",
    "                # print ('E1',Ez_w__x)\n",
    "                line_one = np.ones(shape=(1,1))\n",
    "                Ez_w__x_2=np.vstack((Ez_w__x,line_one))\n",
    "                # print ('AD1',Ez_w__x_2.shape)\n",
    "                #print (np.matmul(beta_k,(data[i]-miu[k])))\n",
    "                Ewzzt_x=(np.identity(Q)-np.matmul(beta_k,W[k])+np.matmul(np.matmul(np.matmul(beta_k,diff),diff.T),beta_k.T))\n",
    "                # print ('E2',Ewzzt_x.shape)\n",
    "                Ewzzt_x2=np.column_stack((np.row_stack((Ewzzt_x,Ez_w__x.T)),Ez_w__x_2))\n",
    "                # print ('AD2',Ewzzt_x2.shape)\n",
    "                # Q=0-p1-p2\n",
    "                # print ('sadsadsa',np.linalg.det(psi))\n",
    "\n",
    "                W_k=np.column_stack((W[k],np.ones(shape=[diff.shape[0],line_one.shape[1]])))\n",
    "\n",
    "                # print ('WKKK',W_k.shape)\n",
    "                # print ('psi',psi.shape)\n",
    "                if np.linalg.det(psi)==0:\n",
    "                    psi1=np.where(psi==0,bias,psi)\n",
    "                    # psi[0][0]=psi[0][0]+bias\n",
    "                else:\n",
    "                    psi1=psi\n",
    "                xx=np.matmul(np.matmul(np.matmul(W_k.T,np.linalg.inv(psi1)),W_k),Ewzzt_x2)\n",
    "                p4=0.5*rnk[i][k]*np.trace(xx)\n",
    "                # print ('PPPPP4',p4)\n",
    "                # print (data[0])\n",
    "                # print (np.matmul(data[i].T,psi))\n",
    "                p2=-0.5*rnk[i][k]*np.matmul(np.matmul(data[i].T,np.linalg.inv(psi)),data[i])\n",
    "                # print ('PPPP2',p2)\n",
    "                p3=-rnk[i][k]*np.matmul(np.matmul(np.matmul(data[i].T,np.linalg.inv(psi)),W_k),Ez_w__x_2)\n",
    "                # print ('PPPPP3',p3)\n",
    "                #jia 1\n",
    "                sum=p2+p3+p4+sum\n",
    "            # sumres=np.sum(rnk_list)  ##求rnk的概率和\n",
    "            # for k in range(K):###归一，做N个样本属于K个类的概率\n",
    "                rnk[i][k]=rnk_list[k]\n",
    "        p1 = -N / 2 * np.log(abs(np.linalg.det(psi)))\n",
    "        # print ('PPPPP1', -p1)\n",
    "        # newloglikelyhood=0.1-p1+sum\n",
    "        # print ('SUM',Q)\n",
    "        ##--------M-step----------------########\n",
    "        W_k_p1_sum=0\n",
    "        Mu_k_p1_sum=0\n",
    "\n",
    "        pai_new_list = []\n",
    "        for k in range(K):\n",
    "        ##更新【W，均值】\n",
    "            ##跟新pai 对i求和\n",
    "            pai_new_sum=0\n",
    "            W_k_news=[]\n",
    "            for i in range(N):\n",
    "\n",
    "                tem = psi + np.matmul(W[k], W[k].T)\n",
    "                if np.linalg.det(tem) == 0:\n",
    "                    tem = np.where(tem == 0, bias, tem)\n",
    "                    # tem[0][0] = tem[0][0] + bias * 0.01\n",
    "                else:\n",
    "                    tem = tem\n",
    "\n",
    "                beta_k = np.matmul(W[k].T, np.linalg.inv(tem))\n",
    "                # print ('beta', beta_k)\n",
    "                # rnk = np.sum( pai[k]* Gaussian(data[i],data[i]-miu[k], (np.matmul(W[k],W[k].T)+psi)))\n",
    "\n",
    "                # print(miu[k].shape)\n",
    "                Ez_w__x = np.matmul(beta_k, (data[i] - miu[k]))\n",
    "                diff = data[i] - miu[k]\n",
    "                # print ('DATA',data[i].shape)\n",
    "                data_i=data[i]\n",
    "                data_i = data_i.reshape(data_i.shape[0], 1)\n",
    "                # print ('DATA',data_i.shape)\n",
    "                Ez_w__x = Ez_w__x.reshape(Ez_w__x.shape[0], 1)\n",
    "                diff = diff.reshape(diff.shape[0], 1)\n",
    "                # print ('E1', Ez_w__x.shape)\n",
    "                line_one = np.ones(shape=(1, 1))\n",
    "                Ez_w__x_2 = np.vstack((Ez_w__x, line_one))\n",
    "                # print ('AD1',Ez_w__x_2.shape)\n",
    "                Ewzzt_x = (np.identity(Q) - np.matmul(beta_k, W[k]) + np.matmul(np.matmul(np.matmul(beta_k, diff), diff.T), beta_k.T))\n",
    "                # print ('E2', Ewzzt_x.shape)\n",
    "                Ewzzt_x2 = np.column_stack((np.row_stack((Ewzzt_x, Ez_w__x.T)), Ez_w__x_2))\n",
    "                # print ('AD',Ewzzt_x2.shape)\n",
    "                W_k_p1_sum=rnk[i][k]*np.matmul(data_i,Ez_w__x_2.T)+W_k_p1_sum\n",
    "                Mu_k_p1_sum=rnk[i][k]*Ewzzt_x2+Mu_k_p1_sum\n",
    "                ###pai的加和\n",
    "                # print ('RNK',rnk[i][k])\n",
    "                pai_new_sum=rnk[i][k]+pai_new_sum\n",
    "            pai_ave=pai_new_sum/N   #####更新PAI\n",
    "            pai_new_list.append(pai_ave)\n",
    "            pai=np.array(pai_new_list)\n",
    "            # print ('PPPAAAAAIII',pai)\n",
    "            W_k_new=np.matmul(W_k_p1_sum,np.linalg.inv(Mu_k_p1_sum))\n",
    "            # print ('一个NEW',W_k_new.shape)\n",
    "            W_k_news.append(W_k_new)\n",
    "            W[k,:,:]=W_k_new[:,:W_k_new.shape[1]-1]\n",
    "            # print ('XIN WWW',W.shape)####更新WWWWW\n",
    "            miu[k,:]=W_k_new[:,-1].T  ####更新MIU!!\n",
    "            # print ('MIU',miu.shape)\n",
    "            # print (\"KKKKK新\",W_k_new.shape)\n",
    "            # print ('MUMUMU新',miu.shape)\n",
    "            # print ('MIU的维度',np.linalg.inv(Mu_k_p1_sum).shape)\n",
    "\n",
    "            # print('WEIDU',W_k_new.shape)\n",
    "\n",
    "        # print ('HEHE', np.sum(pai_new_list))\n",
    "        ##更新协方差矩阵\n",
    "        psi_new_p0=0\n",
    "        ##对i求和\n",
    "        for i in range(N):\n",
    "            ##对 k求和，\n",
    "            psi_new_p1=0\n",
    "            for k in range(K):\n",
    "\n",
    "\n",
    "                data_i = data[i]\n",
    "                data_i = data_i.reshape(data_i.shape[0], 1)\n",
    "                tem = psi + np.matmul(W[k], W[k].T)\n",
    "                if np.linalg.det(tem) == 0:\n",
    "                    tem = np.where(tem == 0, bias, tem)\n",
    "                    tem[0][0] = tem[0][0] + bias * 0.01\n",
    "                else:\n",
    "                    tem = tem\n",
    "\n",
    "                beta_k = np.matmul(W[k].T, np.linalg.inv(tem))\n",
    "                # print ('beta', beta_k)\n",
    "                # rnk = np.sum( pai[k]* Gaussian(data[i],data[i]-miu[k], (np.matmul(W[k],W[k].T)+psi)))\n",
    "\n",
    "                Ez_w__x = np.matmul(beta_k, (data[i] - miu[k]))\n",
    "                diff = data[i] - miu[k]\n",
    "                # print ('DATA', data[i].shape)\n",
    "                data_i = data[i]\n",
    "                data_i = data_i.reshape(data_i.shape[0],1)\n",
    "                # print ('DATA', data_i.shape)\n",
    "                Ez_w__x = Ez_w__x.reshape(Ez_w__x.shape[0], 1)\n",
    "                diff = diff.reshape(diff.shape[0], 1)\n",
    "                # print ('E1', Ez_w__x.shape)\n",
    "                line_one = np.ones(shape=(1, 1))\n",
    "                Ez_w__x_2 = np.vstack((Ez_w__x, line_one))\n",
    "                # print ('AD1', Ez_w__x_2.shape)\n",
    "                # print (np.matmul(np.column_stack((W[k],miu[k])),Ez_w__x_2).shape)\n",
    "                p1=(np.matmul(np.column_stack((W[k],miu[k])),Ez_w__x_2))\n",
    "                # print ('P1',p1.shape,rnk[i][k])\n",
    "                psi_new_p1=rnk[i][k]*np.matmul((data_i-p1),data_i.T)+psi_new_p1\n",
    "            psi_new_p0=psi_new_p1+psi_new_p0\n",
    "        ##最后的取对角线得新的协方差矩阵\n",
    "        # print ('%%%%%%%',psi_new_p0.shape)\n",
    "        #####见论文\n",
    "        psi=np.diag(np.diag(psi_new_p0)) # 更新方差\n",
    "        print (psi[0][0])\n",
    "        # print ('PPPSSSII',Psi_New,np.trace(psi_new_p0))\n",
    "        # rnk_=rnk/sumres\n",
    "        #     r.append(np.sum(rnk))##????????????\n",
    "        # print('每一行数据的和', r)\n",
    "        # # print('dasdas',len(r))\n",
    "        # R.append(r)\n",
    "    # print(np.array(R)[49])\n",
    "    ##计算Q（log_likelihood）\n",
    "\n",
    "        # print ('NEWLOG', newloglikelyhood)\n",
    "\n",
    "    # const=-N/2*log(np.linalg.det(psi))\n",
    "    # part2=0\n",
    "    # # part3=\n",
    "    # for i in range(N):\n",
    "    #     for j in range(K):\n",
    "    #         part2=0.5*rnk*data[i].T*np.linalg.inv(psi)*data[i]+part2\n",
    "\n",
    "    model = None\n",
    "\n",
    "    # You can modify this to save other variables, etc \n",
    "    # but make sure the name of the file is 'model.npz.\n",
    "    np.savez_compressed('model.npz', model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 `[30%]`: Predictions\n",
    "\n",
    "Here we will assume that there is a file `test.npz` from which we will load the test data.  As this file is not given to you, you will need to create one yourself (but not to submit it) to test your code. <span style=\"color:red\">Note that if you do not create this file the cell below will not run</span>. \n",
    "\n",
    "Your task is to fill in the `model_predict` function below. Note that this function should load your `model.npz` file, which must contain all the data structures necessary for making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create these yourself for your own testing but need to delete before submisson\n",
    "x_test = np.random.randn(10000, 784)\n",
    "y_test = np.random.randint(low=0, high=9, size=(10000,1))\n",
    "y_test.shape\n",
    "np.savez('test.npz', x_test=x_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.load('test.npz')\n",
    "x_test = test.get('x_test')\n",
    "y_test = test.get('y_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_ = LabelBinarizer().fit_transform(y_test)\n",
    "y_test_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plot_image_grid(ax, x_test, n=8, m=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(x_test):\n",
    "    \"\"\"\n",
    "    @param x_test: (N_test,D)-array with test data\n",
    "    @return y_pred: (N,C)-array with predicted classes using one-hot-encoding \n",
    "    @return y_log_prob: (N,C)-array with  predicted log probability of the classes   \n",
    "    \"\"\"\n",
    "\n",
    "    # Add your code here: You should load your trained model here \n",
    "    # and write to the corresponding code for making predictions\n",
    "    model = np.load('model.npz');\n",
    "\n",
    "    return y_pred, y_log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 `[20%]`: Performance \n",
    "\n",
    "You do not need to do anything in this section but you can use it to test the generalisation performance of your code. We will use it the evaluate the performance of your algorithm on a new test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance(x_test, y_test, y_pred, y_log_prob):\n",
    "    \"\"\"\n",
    "    @param x_test: (N,D)-array of features \n",
    "    @param y_test: (N,C)-array of one-hot-encoded true classes\n",
    "    @param y_pred: (N,C)-array of one-hot-encoded predicted classes\n",
    "    @param y_log_prob: (N,C)-array of predicted class log probabilities \n",
    "    \"\"\"\n",
    "\n",
    "    acc = np.all(y_test == y_pred, axis=1).mean()\n",
    "    llh = y_log_prob[y_test == 1].mean()\n",
    "\n",
    "    return acc, llh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_log_prob = model_predict(x_test)\n",
    "acc, llh = model_performance(x_test, y_test, y_pred, y_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Average test accuracy=' + str(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Average test likelihood=' + str(llh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print ('xx')\n",
    "    x=model_train(x_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
